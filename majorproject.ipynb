{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "majorproject.ipynb",
      "provenance": [],
      "mount_file_id": "1H0jQI_g1dZym0ItCGJF6eAM0JEI3povE",
      "authorship_tag": "ABX9TyP7fH2UTzE+HcDHnvCIgMxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ezhil531/Sentiment-analysis-using-product-review-data/blob/main/majorproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHjcnHJNrkeF",
        "outputId": "59e93350-e3f3-4eb5-bac6-2c3ebcc371aa"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\"\"\"Importing Dataset\"\"\"\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/dataset')\n",
        "\n",
        "# Taking only useful data from dataset\n",
        "dataset = dataset.iloc[:,[17,16,14,11]].values\n",
        "\n",
        "\"\"\"Taking Care of missing data\"\"\"\n",
        "\n",
        "# Discarding observations having missing data\n",
        "data = pd.DataFrame(dataset)\n",
        "dataset = data.dropna(axis = 0,how = 'any')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "\n",
        "# Storing rating saperately to use it later \n",
        "rating = X[:,-1]\n",
        "\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Encoding True/False value as 1/0\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "\"\"\"Visualizing Dataset\"\"\"\n",
        "\n",
        "dataset.head(-1)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "\"\"\"Cleaning the Dataset\"\"\"\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "# Downloading stopwords which does not having much significance\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemmer will reduce words in their root form\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "all_stopwords = stopwords.words('english')\n",
        "\n",
        "# Removing some stopwords which have significance effect in building this model\n",
        "rem = ['not', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \n",
        "       \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
        "       \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'don', \"don't\", \n",
        "       'just', 'too', 'very', 'no', 'nor', 'only', 'own', 'same', 'again', 'against', 'but',]\n",
        "for s in rem:\n",
        "  all_stopwords.remove(s)\n",
        "\n",
        "def find_clean_text(temp):\n",
        "  # Removing all characters other than alphabet\n",
        "  temp = re.sub('[^a-zA-Z]', ' ', temp)\n",
        "  temp = temp.lower()\n",
        "  temp = temp.split()\n",
        "  temp = [ps.stem(word) for word in temp if not word in set(all_stopwords)]\n",
        "  temp = ' '.join(temp)\n",
        "  return temp\n",
        "\n",
        "corpus = []\n",
        "for i in range(X.shape[0]):\n",
        "  # Concatanating both title and detailed review\n",
        "  temp = X[i][0] + ' ' + X[i][1]\n",
        "  temp = find_clean_text(temp)\n",
        "  corpus.append(temp)\n",
        "\n",
        "\"\"\"Creating the Bag of Words model\"\"\"\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 3000)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "# Adding rating in the matrix of feature X\n",
        "rating = rating.reshape(rating.shape[0],1)\n",
        "X = np.append(X,rating,axis=1)\n",
        "\n",
        "\"\"\"Splitting the dataset into the Training set and Test set\"\"\"\n",
        "\n",
        "# Splitting dataset into test set and train set which have equal percentage of data with both positive and negative review\n",
        "#This is done as precautionary measure considering that very small number (approx 1300 out of 34000) data have negative review.\n",
        "pos_x = []\n",
        "pos_y = []\n",
        "neg_x = []\n",
        "neg_y = []\n",
        "for i in range(X.shape[0]):\n",
        "  if y[i]==1:\n",
        "    pos_x.append(X[i])\n",
        "    pos_y.append(y[i])\n",
        "  else:\n",
        "    neg_x.append(X[i])\n",
        "    neg_y.append(y[i])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(pos_x, pos_y, test_size = 0.20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(neg_x, neg_y, test_size = 0.20)\n",
        "\n",
        "for i in range(len(X_train1)):\n",
        "  X_train.append(X_train1[i])\n",
        "  y_train.append(y_train1[i])\n",
        "for i in range(len(X_test1)):\n",
        "  X_test.append(X_test1[i])\n",
        "  y_test.append(y_test1[i])\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "\"\"\"Training the Multinomial Naive Bayes model on the Training set\"\"\"\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "\"\"\"Making the Confusion Matrix\"\"\"\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "print('Result on training set :')\n",
        "print('Confusion matrix :')\n",
        "print(confusion_matrix(y_train,classifier.predict(X_train)))\n",
        "print('accuracy : ',accuracy_score(y_train, classifier.predict(X_train)))\n",
        "\n",
        "print('Result on test set :')\n",
        "y_pred = classifier.predict(X_test)\n",
        "print('Confusion matrix :')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print('accuracy',accuracy_score(y_test, y_pred))\n",
        "\n",
        "\"\"\"Making Prediction on some reviews from test set\"\"\"\n",
        "\n",
        "random.seed(13)\n",
        "index = random.randrange(dataset.shape[0])\n",
        "print('Title :',dataset[0][index],'\\nReview :',dataset[1][index],'\\nRating :',dataset[2][index])\n",
        "print(\"True value :\", dataset[3][index])\n",
        "print(\"Prediction :\",classifier.predict(np.append(cv.transform([find_clean_text(dataset[0][index]+' '+dataset[1][index])]).toarray(),[[dataset[3][index]]],axis=1)))\n",
        "\n",
        "index = random.randrange(dataset.shape[0])\n",
        "print('Title :',dataset[0][index],'\\nReview :',dataset[1][index],'\\nRating :',dataset[2][index])\n",
        "print(\"True value :\", dataset[3][index])\n",
        "print(\"Prediction :\",classifier.predict(np.append(cv.transform([find_clean_text(dataset[0][index]+' '+dataset[1][index])]).toarray(),[[dataset[3][index]]],axis=1)))\n",
        "\n",
        "index = random.randrange(dataset.shape[0])\n",
        "print('Title :',dataset[0][index],'\\nReview :',dataset[1][index],'\\nRating :',dataset[2][index])\n",
        "print(\"True value :\", dataset[3][index])\n",
        "print(\"Prediction :\",classifier.predict(np.append(cv.transform([find_clean_text(dataset[0][index]+' '+dataset[1][index])]).toarray(),[[dataset[3][index]]],axis=1)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['Kindle'\n",
            "  'This product so far has not disappointed. My children love to use it and I like the ability to monitor control what content they see with ease.'\n",
            "  5.0]\n",
            " ['very fast'\n",
            "  'great for beginner or experienced person. Bought as a gift and she loves it'\n",
            "  5.0]\n",
            " ['Beginner tablet for our 9 year old son.'\n",
            "  'Inexpensive tablet for him to use and learn on, step up from the NABI. He was thrilled with it, learn how to Skype on it already...'\n",
            "  5.0]\n",
            " ...\n",
            " ['Love it'\n",
            "  'Simply the best to watch tv series and movies. It works even better if you are an Amazon Prime subscriber, with access to a many free goodies.'\n",
            "  5.0]\n",
            " ['Try it, you will like it'\n",
            "  'I was looking for ways to cut cost from a raising cable bill and a friend suggested I try the Amazon Fire. At first I didn���t know if this was something I could do. Once I was able to maneuver through the process, I love it.'\n",
            "  4.0]\n",
            " ['Great little device'\n",
            "  'I enjoy my kindle tv, it beats paying for cable every month ������'\n",
            "  4.0]]\n",
            "[1 1 1 ... 1 1 1]\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Result on training set :\n",
            "Confusion matrix :\n",
            "[[  782   325]\n",
            " [  903 25239]]\n",
            "accuracy :  0.9549341260229733\n",
            "Result on test set :\n",
            "Confusion matrix :\n",
            "[[ 179   98]\n",
            " [ 240 6296]]\n",
            "accuracy 0.950388962277998\n",
            "Title : My 3 yr old grandson loves it! \n",
            "Review : My grandson really enjoys this tablet, ease of use and lots of different educational programs. Must remember to charge tablet everynight. \n",
            "Rating : 5.0\n",
            "True value : True\n",
            "Prediction : [1]\n",
            "Title : Makes reading at night great \n",
            "Review : More positives than negatives on this one. Battery is great, readability is great. The backlight makes low light reading super. The only thing that would make this even better would be water resistance. \n",
            "Rating : 4.0\n",
            "True value : True\n",
            "Prediction : [1]\n",
            "Title : Hate Amazon \n",
            "Review : Hate Amazon! The app store doesn't have cool apps! \n",
            "Rating : 3.0\n",
            "True value : False\n",
            "Prediction : [0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}